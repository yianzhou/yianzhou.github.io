# 直播推流

最近由于项目需要，研究了一下终端到服务端的视频推流，在此记录分享一下。

## 业务场景

我遇到的业务场景和视频直播中的推流/拉流很像，典型的架构如图：

对于直播推拉流，已有很多成熟的方案，例如微信视频号直播用的 RTMP。但这里我只需要将客户端的直播画面推送到服务端，后续由服务端交给大模型进行理解，不需要拉流分发。所以我决定自己实现一下这个过程。

首先是客户端。采集直播画面可以取手机的摄像头画面，也可以取手机的屏幕直播画面。

对于摄像头采集，常规的摄像头代码就不贴了，只需留意添加`videoOutput`时，将`videoSetting`设置为如下形式：

```c
videoOutput.videoSettings = @{
    (id)kCVPixelBufferPixelFormatTypeKey : @(kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange),
};
```

`kCVPixelBufferPixelFormatTypeKey` 的设置直接决定了你拿到的视频帧的像素格式、颜色空间和内存布局。它决定了每个像素的颜色数据是如何编码和存储的。主要有两种选择：YUV 和 BGRA。

`kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange` 是 iOS 摄像头硬件直接输出的格式，由于色度抽样，相比未压缩的 RGB 格式，数据量更小，适合网络传输和存储。硬件视频编码器（如 H.264）的输入格式就是 YUV。对于录制视频或进行视频流传输来说，这是最高效的选择。

`kCVPixelFormatType_32BGRA`是未压缩的 RGB 颜色格式。由于摄像头原生输出是 YUV，将每一帧 YUV 数据转换为 BGRA 会增加 CPU/GPU 资源的消耗。适合需要对视频帧进行实时图像处理（如美颜、滤镜、贴纸）的情况。

对于手机屏幕直播画面，使用 `Broadcast Upload Extension` 可以采集到系统级别的手机屏幕画面。

这两种采集方式都能收到包含 `CMSampleBuffer` 数据的回调，这个是苹果提供的帧数据结构，包含特定类型（音频、视频、混合等）的媒体样本。

## 编码

接下来是重点的 H264 编码部分。收到第一帧 `CMSampleBuffer` 后，开启一个 `VTCompressionSession` 会话，`VTCompressionSession` 是苹果 VideoToolbox 提供的硬件编码能力。

```c
CMFormatDescriptionRef formatDesc = CMSampleBufferGetFormatDescription(sampleBuffer);
CMVideoDimensions dims = CMVideoFormatDescriptionGetDimensions(formatDesc);
int width = dims.width;
int height = dims.height;

// 创建视频压缩会话，指定宽、高，指定H264编码格式，指定compressionOutputCallback为编码回调函数
OSStatus status = VTCompressionSessionCreate(NULL, width, height, kCMVideoCodecType_H264, NULL, NULL, NULL, compressionOutputCallback, (__bridge void *)self, &_compressionSession);
```

创建会话后，使用`VTSessionSetProperty`设置会话相关的属性，例如关键帧间隔、ProfileLevel、帧率、比特率等。

关键帧（Instantaneous Decoder Refresh, IDR, 也称为 I 帧) 是一张完整的图像，就像一张可完整显示的图片。解码器只需要这一帧的数据就能完整显示整个画面；而非关键帧 (Predicted Frame, P 帧和 Bi-directional Predicted Frame, B 帧) 也叫“增量帧”或“差异帧”，它们不包含完整的图像信息，只记录了与前后帧相比发生了变化的部分，这样编码可以极大地提高压缩率。

视频流就是由一个关键帧，后面跟着一串非关键帧，然后又一个关键帧……这样组成的。这个关键帧的序列被称为 GOP (Group of Pictures)。`kVTCompressionPropertyKey_MaxKeyFrameInterval` 就定义了这个 GOP 的最大长度，也就是两个关键帧之间最多允许有多少帧。对于直播推流 30FPS 来说，每 30 帧一个关键帧是比较合适的。

在 H.264 编码标准中，Profile 和 Level 是两个独立的参数。

Profile 定义了编码器在压缩视频时可以使用编码特性：

- Baseline Profile: 最简单，包含的特性最少。不支持 B 帧（双向预测帧），不支持 CABAC 熵编码。压缩率最低，但解码时计算量最小，延迟也最低。
- Main Profile: 在 Baseline 的基础上增加了 B 帧和 CABAC 等，大大提高了压缩效率。
- High Profile: 在 Main 的基础上增加了更多高级特性，如 8x8 内部预测、自定义量化矩阵等。是目前压缩率最高的 Profile。

Level 定义了解码器必须能够处理的视频码率、分辨率、帧率等参数的上限。例如 4.0 表示 1920x1080@30fps(1080p)。

这里我们将`kVTCompressionPropertyKey_ProfileLevel`设置为`kVTProfileLevel_H264_High_AutoLevel`，意思是使用 High Profile，Level 让框架根据分辨率和帧率自动判断。

`kVTCompressionPropertyKey_AverageBitRate` 这个属性设置的是平均码率 (Average Bit Rate)，对应的是 VBR (Variable Bit Rate) 可变码率，当画面变化小时，它会使用低于 4 Mbps 的码率，从而节省带宽；当画面变化大时，它会瞬时使用高于 4 Mbps 的码率，以保证画面的清晰度。对于直播 1080p 视频来说，设置 4500 Kbps 左右的码率比较合适（大约是`1920*1080*2`）。

使用 `kVTCompressionPropertyKey_DataRateLimits` 可以给编码器增加一个硬性上限，即不管画面瞬时变化有多大，我们希望每个时刻的码率都不超过 6 Mbps。

```c
// 设置帧率
int fps = 30;
CFNumberRef fpsNum = CFNumberCreate(kCFAllocatorDefault, kCFNumberIntType, &fps);
VTSessionSetProperty(self.compressionSession, kVTCompressionPropertyKey_ExpectedFrameRate, fpsNum);
CFRelease(fpsNum);

// 设置关键帧间隔
int keyFrameInterval = 30;
CFNumberRef keyFrameIntervalNum = CFNumberCreate(kCFAllocatorDefault, kCFNumberIntType, &keyFrameInterval);
VTSessionSetProperty(self.compressionSession, kVTCompressionPropertyKey_MaxKeyFrameInterval, keyFrameIntervalNum);
CFRelease(keyFrameIntervalNum);

// 设置Profile Level
VTSessionSetProperty(self.compressionSession, kVTCompressionPropertyKey_ProfileLevel, kVTProfileLevel_H264_Main_AutoLevel);

// 告诉编码器这是一个实时直播的画面，优先保证低延迟和处理速度，可以适当牺牲一些压缩率和画面质量。
VTSessionSetProperty(self.compressionSession, kVTCompressionPropertyKey_RealTime, kCFBooleanTrue);

// 设置平均码率
int32_t averageBitRate = 4500 * 1000; // 4,500,000 bits per second
CFNumberRef bitRateNum = CFNumberCreate(kCFAllocatorDefault, kCFNumberSInt32Type, &averageBitRate);
VTSessionSetProperty(self.compressionSession, kVTCompressionPropertyKey_AverageBitRate, bitRateNum);
CFRelease(bitRateNum);

// 设置码率上限
int dataLimitBytes = 6000 * 1000 / 8; // 6 Mbps in Bytes per second
int dataLimitSeconds = 1.0;
VTSessionSetProperty(self.compressionSession, kVTCompressionPropertyKey_DataRateLimits, (__bridge CFArrayRef)@[@(dataLimitBytes), @(dataLimitSeconds)]);
```

参数准备就绪后，调用 `VTCompressionSessionPrepareToEncodeFrames(self.compressionSession)` 准备编码。

然后，将流式输入的 `CMSampleBuffer` 传给 `VTCompressionSessionEncodeFrame` 函数进行编码。这是一个异步函数，它会立刻返回，并在编码完成后将结果通过你在创建 `VTCompressionSession` 时注册的回调函数返回给你。

```c
self.frameCount++;
CVImageBufferRef imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);
CMTime presentationTimeStamp = CMTimeMake(self.frameCount, 30);
CMTime duration = CMSampleBufferGetDuration(sampleBuffer);
VTCompressionSessionEncodeFrame(self.compressionSession, imageBuffer, presentationTimeStamp, duration, NULL, NULL, NULL);
```

## 数据格式

NALU (Network Abstraction Layer Unit) 是 H.264/HEVC 标准定义的一个基本数据包格式。H.264 视频流就是一个个 NALU 拼接起来的。

一个 NALU 由两部分组成：

- NAL Header: 其中 NALU Type 指明了这个 NALU 的类型（IDR 帧、非 IDR 帧、SPS、PPS）
- NAL Payload: 实际的视频数据。对于 SPS/PPS，这里面是配置信息；对于 IDR/P/B 帧，这里面是压缩后的图像数据。

SPS (Sequence Parameter Set) 和 PPS (Picture Parameter Set) 是 H.264/HEVC 编码标准中定义的两种参数集 NALU。它们不包含像素数据，而是包含解码器正确解析像素数据所必需的元数据。

SPS 是为整个视频文件、一个直播流设定的参数，包括 Profile、Level、图像尺寸、帧率和时序等，在一个视频流中固定不变。如果 SPS 发生改变，通常意味着视频的核心属性如分辨率发生了变化（例如用户在直播中切换了清晰度），此时解码器需要重新进行初始化。

PPS 是被多个图像共享的参数，包括量化、熵编码等，在一个视频序列中可以存在多个不同的 PPS。

H.264 数据在实际应用中主要有两种封装格式：

第一种是 AVCC 格式 (或 Length-Prefixed Format, 长度前缀格式) ，它的结构如下：

`[4-byte NALU Length][NALU Data][4-byte NALU Length][NALU Data]...`

它的特点是每个 NALU 单元前面都有一个 4 字节的大端、整数，用来指明这个 NALU 的长度。解码器可以先读取 4 个字节来知道接下来要读取多少数据。这是 VideoToolbox 框架默认输出的格式，也是 `.mp4`, `.mov` 等文件容器内部存储 H.264 数据时使用的格式。

第二种是 Annex B 格式 (或 Start Code-Prefixed Format, 起始码前缀格式)，它的结构如下：

`[Start Code][NALU Data][Start Code][NALU Data]...`

它的特点是每个 NALU 单元前面都有一个起始码 (Start Code)，通常是 `0x00000001` (4 字节)。这种格式使得在传输数据流时，解析方可以在一个连续的字节流中通过起始码来找到每个 NALU 的边界，被广泛用于流媒体传输协议中。

## 推流

我们将`CMSampleBuffer`提交给`VTCompressionSession`压缩后，输出的数据是 AVCC 格式，这里我们需要转换成网络传输所需要的 Annex B 格式。转换代码如下：

```c
// VTCompressionSession编码函数的回调
static void compressionOutputCallback(void *outputCallbackRefCon,
                                      void *sourceFrameRefCon,
                                      OSStatus status,
                                      VTEncodeInfoFlags infoFlags,
                                      CMSampleBufferRef sampleBuffer) {
    if (status != noErr) {
        return;
    }

    H264Encoder *encoder = (__bridge H264Encoder *)outputCallbackRefCon;
    if (!CMSampleBufferDataIsReady(sampleBuffer)) {
        return;
    }

    // 判断是否关键帧
    BOOL isKeyFrame = isKeyframeFromSampleBuffer(sampleBuffer);

    // 如果判断当前帧是关键帧，需要额外从`CMFormatDescriptionRef`中提取 SPS/PPS 信息发送
    if (isKeyFrame) {
        sendSPSAndPPSFromSampleBuffer(sampleBuffer, encoder);
    }

    // 将AVCC格式数据封装成 annex B 格式的视频流数据
    CMBlockBufferRef blockBuffer = CMSampleBufferGetDataBuffer(sampleBuffer);
    size_t totalLength = 0;
    size_t offset = 0;
    char *dataPointer = NULL;
    OSStatus statusCodeRet = CMBlockBufferGetDataPointer(blockBuffer, 0, NULL, &totalLength, &dataPointer);
    if (statusCodeRet != noErr) {
        return;
    }

    // 先读取4字节的NALU长度，然后读取视频数据，如此循环
    while (offset + 4 < totalLength) {
        uint32_t nalUnitLength = 0;
        memcpy(&nalUnitLength, dataPointer + offset, 4);
        nalUnitLength = CFSwapInt32BigToHost(nalUnitLength);

        // 只有在这个 NALU 的完整数据确实存在于缓冲区中时，才继续处理
        if (offset + 4 + nalUnitLength <= totalLength) {
            NSData *nalData = [NSData dataWithBytes:dataPointer + offset + 4 length:nalUnitLength];
            NSMutableData *annexB = [NSMutableData dataWithBytes:kNALStartCode length:4];
            [annexB appendData:nalData];
            if ([encoder.delegate respondsToSelector:@selector(encoderDidOutputData:)]) {
                [encoder.delegate encoderDidOutputData:annexB];
            }
            // 更新偏移量
            offset += 4 + nalUnitLength;
        } else {
            // 如果 NALU 长度值异常，说明数据已损坏。
            NSLog(@"Error: Corrupted NAL unit length detected. Aborting parsing.");
            break;
        }
    }
}

// 判断是否是关键帧
static BOOL isKeyframeFromSampleBuffer(CMSampleBufferRef sampleBuffer) {
    if (!sampleBuffer) {
        return NO;
    }

    CFArrayRef attachments = CMSampleBufferGetSampleAttachmentsArray(sampleBuffer, false);
    if (!attachments || CFArrayGetCount(attachments) == 0) {
        return NO;
    }

    CFDictionaryRef attachmentDict = CFArrayGetValueAtIndex(attachments, 0);
    if (!attachmentDict) {
        return NO;
    }

    // The key kCMSampleAttachmentKey_NotSync indicates a non-keyframe.
    // If this key is absent, it's a keyframe.
    return !CFDictionaryContainsKey(attachmentDict, kCMSampleAttachmentKey_NotSync);
}

// 如果是关键帧，提取SPS和PPS数据并发送
static void sendSPSAndPPSFromSampleBuffer(CMSampleBufferRef sampleBuffer, H264Encoder *encoder) {
    if (!sampleBuffer || !encoder) {
        return;
    }

    // 1. Get the format description from the sample buffer.
    CMFormatDescriptionRef format = CMSampleBufferGetFormatDescription(sampleBuffer);
    if (!format) {
        return;
    }

    // 2. Extract SPS.
    size_t spsSize, spsCount;
    const uint8_t *sps = NULL;
    OSStatus spsStatus = CMVideoFormatDescriptionGetH264ParameterSetAtIndex(format, 0, &sps, &spsSize, &spsCount, NULL);

    // 3. Extract PPS.
    size_t ppsSize, ppsCount;
    const uint8_t *pps = NULL;
    OSStatus ppsStatus = CMVideoFormatDescriptionGetH264ParameterSetAtIndex(format, 1, &pps, &ppsSize, &ppsCount, NULL);

    // 4. If both were extracted successfully, package and send them.
    if (spsStatus == noErr && ppsStatus == noErr) {
        // Prepare SPS NAL unit in Annex B format.
        const uint8_t kNALStartCode[] = {0x00, 0x00, 0x00, 0x01};
        NSMutableData *spsData = [NSMutableData dataWithBytes:kNALStartCode length:4];
        [spsData appendBytes:sps length:spsSize];
        // Send via delegate.
        if ([encoder.delegate respondsToSelector:@selector(encoderDidOutputData:)]) {
            [encoder.delegate encoderDidOutputData:spsData];
        }

        // Prepare PPS NAL unit in Annex B format.
        NSMutableData *ppsData = [NSMutableData dataWithBytes:kNALStartCode length:4];
        [ppsData appendBytes:pps length:ppsSize];
        // Send via delegate.
        if ([encoder.delegate respondsToSelector:@selector(encoderDidOutputData:)]) {
            [encoder.delegate encoderDidOutputData:ppsData];
        }
    }
}
```

VideoToolbox 不会将 SPS 和 PPS 作为 NALU 直接放在视频数据流中。它将 SPS/PPS 信息存储在 `CMSampleBuffer` 的 `CMFormatDescriptionRef` 中。这些参数集会在第一个关键帧以及后续任何格式发生变化的关键帧的格式描述中提供。所以如果判断当前帧是关键帧，需要额外从`CMFormatDescriptionRef`中提取 SPS/PPS 信息发送。

推流这里 Demo 用的是 WebSocket 方案，可以使用第三方库`SocketRocket`，也可以使用原生 API `NSURLSessionWebSocketTask`。这部分代码比较简单因此省略。

## 解码

我的 Demo 使用了 Python 作为服务端通过 WebSocket 接收数据，并交给 FFmpeg 解码保存视频文件。

```py
#!/usr/bin/env python

import asyncio
from pathlib import Path

from websockets.asyncio.server import serve

# --- Configuration ---
SEGMENT_DIR = Path("segments")
SEGMENT_DURATION_SECONDS = 3


async def handle_h264_stream(websocket):
    """
    Handle a websocket, pipe its binary data to ffmpeg for segmentation.
    This is a "happy path" version with minimal error handling.
    """
    print("[ws] client connected, starting ffmpeg")

    # Ensure the output directory exists
    SEGMENT_DIR.mkdir(parents=True, exist_ok=True)
    output_pattern = str(SEGMENT_DIR / "part_%06d.ts")

    # Start the ffmpeg process
    ffmpeg_proc = await asyncio.create_subprocess_exec(
        "ffmpeg",
        "-hide_banner",
        "-loglevel",
        "error",
        "-fflags",
        "+genpts",
        "-use_wallclock_as_timestamps",
        "1",
        "-r",
        "30",
        "-f",
        "h264",
        "-i",
        "pipe:0",
        "-c",
        "copy",
        "-f",
        "segment",
        "-segment_time",
        str(SEGMENT_DURATION_SECONDS),
        "-segment_format",
        "mpeg_ts",
        "-reset_timestamps",
        "1",
        output_pattern,
        stdin=asyncio.subprocess.PIPE,
        # We don't read stdout/stderr
        stdout=asyncio.subprocess.DEVNULL,
        stderr=asyncio.subprocess.DEVNULL,
    )

    # Main loop to forward messages
    async for message in websocket:
        if isinstance(message, (bytes, bytearray)):
            print(f"[ws] received {len(message)} bytes")
            # Pipe binary data directly to ffmpeg's stdin
            ffmpeg_proc.stdin.write(message)
            await ffmpeg_proc.stdin.drain()

    # When the websocket closes, close ffmpeg's stdin to signal end of stream
    print("[ws] client disconnected, closing ffmpeg")
    if ffmpeg_proc.stdin:
        ffmpeg_proc.stdin.close()

    # Wait for the ffmpeg process to terminate
    await ffmpeg_proc.wait()
    print(f"[ffmpeg] exited with code {ffmpeg_proc.returncode}")


async def main():
    """Main function to start the WebSocket server."""
    async with serve(handle_h264_stream, "0.0.0.0", 8000):
        print("WebSocket server listening on ws://0.0.0.0:8000")
        await asyncio.Event().wait()  # A simple way to run forever


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nServer stopped.")
```

## 问题

收到客户端上传的画面后，打开 ts 文件，发现有两个问题：1. 画面刚开始的几帧总是特别暗；2. 画面旋转了 90 度。
